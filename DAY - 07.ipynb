{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9834c61-96b4-4fe0-8a7a-a5d50b94ee2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"MLflow setup successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e440c1-8c48-4b09-a1f7-bfec7e87c00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reload bronze events table\n",
    "events = spark.table(\"workspace.ecommerce.events_delta\")\n",
    "\n",
    "# Reload silver feature table\n",
    "features_df = spark.table(\"workspace.ecommerce.silver_user_features\")\n",
    "\n",
    "# Recreate binary label (target)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "label_df = events.groupBy(\"user_id\").agg(\n",
    "    F.max(\n",
    "        F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)\n",
    "    ).alias(\"purchased\")\n",
    ")\n",
    "# Join features + label (same as Day 5/6)\n",
    "training_data = features_df.join(label_df, \"user_id\")\n",
    "\n",
    "print(\"Training dataset recreated successfully!\")\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd21d952-3dfc-4d69-9821-210953845500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Recreate train/test split\n",
    "train_df, test_df = training_data.randomSplit([0.8, 0.2], seed=42)\n",
    "# Create feature assembler (same as Day 6)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_events\", \"total_purchases\", \"total_spent\", \"avg_spent\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "# Create ML-ready datasets\n",
    "train_ml = assembler.transform(train_df) \\\n",
    ".select(\"features\", F.col(\"purchased\").alias(\"label\"))\n",
    "\n",
    "test_ml = assembler.transform(test_df) \\\n",
    ".select(\"features\", F.col(\"purchased\").alias(\"label\"))\n",
    "\n",
    "print(\"Train/Test ML datasets prepared successfully!\")\n",
    "print(\"Train ML count:\", train_ml.count())\n",
    "print(\"Test ML count:\", test_ml.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51468900-6c91-495f-8a37-7d531773f91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "rf_model = rf.fit(train_ml)\n",
    "print(\"RandomForest model trained for MLflow logging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04b76554-741e-42c4-a40e-6bbe8fb832fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Generate predictions on test data\n",
    "predictions = rf_model.transform(test_ml)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Calculate AUC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"AUC calculated for MLflow logging:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0cf81b-5143-40c9-a01f-2d932877499c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_ml.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc39d78d-0fbb-4fcd-ba7b-8a7e5241bbad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Recreate predictions safely\n",
    "predictions = rf_model.transform(test_ml)\n",
    "\n",
    "# Check prediction sample\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "# Correct evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Correct AUC for MLflow logging:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3f1185-5734-42eb-a045-28c641d189c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set MLflow temp directory to your UC Volume path\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/ecommerce_data/mlflow_tmp\"\n",
    "\n",
    "print(\"MLflow UC volume temp path configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016c9379-9e60-45db-8295-303efe06aefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Day7_RandomForest_Model\"):\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"num_trees\", 50)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"features\", [\"total_events\", \"purchases\", \"total_spent\", \"avg_price\"])\n",
    "\n",
    "    # Log metric (AUC)\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # Log Spark ML model\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=rf_model,\n",
    "        artifact_path=\"random_forest_model\"\n",
    "    )\n",
    "\n",
    "print (\"MLflow run logged successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY - 07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
